projectName: lentra-los
default:
  annotations:
    all:
      app: lentra-los
  labels:
    all:
      "app.kubernetes.io/part-of": lentra-los
  image:
    repository: "183295448290.dkr.ecr.ap-south-1.amazonaws.com/atlas/lentra-los"
    tag: 0.0.0
  resources:
    requests:
      memory: 256Mi
    limits:
      memory: 1024Mi
  externalSecrets:
    injection:
      secrets:
        - secretName: env
          dataFrom:
            secretKey: app/lentra-los
          refreshInterval: 30s
          secretStoreName: aws-secret-manager

deployments:
  - fullnameOverride: lentra-los
    serviceAccount:
      create: true
      annotations:
        eks.amazonaws.com/role-arn: "arn:aws:iam::183295448290:role/atlas-api-server-role"
    service:
      type: NodePort
      ports:
        - port: 8000
          name: api
          ingress:
            enabled: true
            annotations:
               kubernetes.io/ingress.class: alb
               alb.ingress.kubernetes.io/group.name: atlas
               alb.ingress.kubernetes.io/target-type: instance
               alb.ingress.kubernetes.io/scheme: internet-facing
               alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80},{"HTTPS": 443}]'
               alb.ingress.kubernetes.io/ssl-redirect: '443'
               alb.ingress.kubernetes.io/healthcheck-path: /v3/verification/health
               alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '15'
               alb.ingress.kubernetes.io/healthcheck-interval-seconds: '300'
               alb.ingress.kubernetes.io/load-balancer-attributes: idle_timeout.timeout_seconds=300
               alb.ingress.kubernetes.io/target-group-attributes: deregistration_delay.timeout_seconds=300
               alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-south-1:183295448290:certificate/9935b050-cc72-465d-9afa-9975803f6a3f
            rules:
            - host: lentra-loss.dev.kreditmind.com
              http:
                paths:
                  - path: /*
                    pathType: ImplementationSpecific
                    backend:
                      service:
                        name: lentra-los
                        port:
                          number: 8000
    containers:
      - fullnameOverride: lentra-los
        port: 8000
        imagePullPolicy: Always
        env:
          APP_MODE: "web"
        resources:
          requests:
            memory: 128Mi
        healthcheck:
          enabled: true
          livenessProbe:
            path: "/v3/verification/health"
            initialDelaySeconds: 10
            periodSeconds: 20
          readinessProbe:
            path: "/v3/verification/health"
            initialDelaySeconds: 10
            periodSeconds: 20
  
  - fullnameOverride: lentra-loss-worker-callback-worker
    serviceAccount:
      create: true
      annotations:
        eks.amazonaws.com/role-arn: "arn:aws:iam::183295448290:role/atlas-api-server-role"
    keda:
      enabled: true
      min: 1
      max: 6
      pollingInterval: 30
      cooldownPeriod: 300
      triggers:
        - type: aws-sqs-queue
          metadata:
            queueURL: https://sqs.ap-south-1.amazonaws.com/183295448290/uat-lentra-los-callback
            queueLength: "30"
            awsRegion: ap-south-1
            identityOwner: operator
    containers:
      - fullnameOverride: lentra-loss-worker-callback-worker
        imagePullPolicy: Always
        env:
          APP_MODE: "worker"
          CELERY_QUEUES: "uat-lentra-los-callback"
          CELERY_CONCURRENCY: "5"
          CELERY_LOGLEVEL: "info"
        resources:
          requests:
            memory: 128Mi
  - fullnameOverride: lentra-los-celery-crosschecks
    serviceAccount:
      create: true
      annotations:
        eks.amazonaws.com/role-arn: "arn:aws:iam::183295448290:role/atlas-api-server-role"
    containers:
      - fullnameOverride: lentra-los-celery-crosschecks
        imagePullPolicy: Always
        env:
          APP_MODE: "worker"
          CELERY_QUEUES: "uat-lentra-los-crosschecks"
          CELERY_CONCURRENCY: "5"
          CELERY_LOGLEVEL: "info"
        resources:
          requests:
            memory: 128Mi
  - fullnameOverride: lentra-los-worker-llmservice
    serviceAccount:
      create: true
      annotations:
        eks.amazonaws.com/role-arn: "arn:aws:iam::183295448290:role/atlas-api-server-role"
    containers:
      - fullnameOverride: lentra-los-worker-llmservice
        imagePullPolicy: Always
        env:
          APP_MODE: "worker"
          CELERY_QUEUES: "uat-lentra-los-llmservice"
          CELERY_CONCURRENCY: "5"
          CELERY_LOGLEVEL: "info"
        resources:
          requests:
            memory: 128Mi
          limits:
            memory: 1024Mi
  # - fullnameOverride: lentra-loss-worker-postllm
  #   serviceAccount:
  #     create: true
  #     annotations:
  #       eks.amazonaws.com/role-arn: "arn:aws:iam::183295448290:role/atlas-api-server-role"
  #   containers:
  #     - fullnameOverride: lentra-loss-worker-chat-postllm
  #       imagePullPolicy: Always
  #       env:
  #         APP_MODE: "worker"
  #         CELERY_QUEUES: "uat-lentra-los-postllm"
  #         CELERY_CONCURRENCY: "5"
  #         CELERY_LOGLEVEL: "info"
  #       resources:
  #         requests:
  #           memory: 128Mi
  - fullnameOverride: lentra-loss-worker-llm-service
    serviceAccount:
      create: true
      annotations:
        eks.amazonaws.com/role-arn: "arn:aws:iam::183295448290:role/atlas-api-server-role"
    containers:
      - fullnameOverride: llm-service
        imagePullPolicy: Always
        env:
          APP_MODE: "worker"
          CELERY_QUEUES: "uat-lentra-los-llmservice"
          CELERY_CONCURRENCY: "5"
          CELERY_LOGLEVEL: "info"
        resources:
          requests:
            memory: 128Mi
jobs:
  - name: dbmigrate-job
    serviceAccount:
      create: true
      annotations:
        eks.amazonaws.com/role-arn: "arn:aws:iam::183295448290:role/atlas-api-server-role"
    annotations:
      "helm.sh/hook": pre-upgrade,pre-install
      "helm.sh/hook-weight": "-5"
      "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    containers:
      - name: "dbmigrate-job"
        env:
          APP_MODE: "migration"
        command: ["flask", "db", "upgrade"]
        resources:
          requests:
            memory: 512Mi
          limits:
            memory: 512Mi
